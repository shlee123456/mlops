# ν”„λ΅μ νΈ ν„ν™© λ° λ΅λ“λ§µ

μ‘μ„±μΌ: 2025-12-11

## ν”„λ΅μ νΈ κ°μ”

**λ©ν‘:** LLM Fine-tuning + End-to-End MLOps νμ΄ν”„λΌμΈ κµ¬μ¶•

**ν•µμ‹¬ κΈ°μ :**
- LLM: Mistral-7B / LLaMA-2-7B
- Fine-tuning: LoRA / QLoRA (PEFT)
- Serving: vLLM, FastAPI
- MLOps: MLflow, DVC, Docker
- λ¨λ‹ν„°λ§: Prometheus + Grafana

---

## μ™„λ£λ μ‘μ—… β…

### Phase 0: ν”„λ΅μ νΈ μ΄κΈ°ν™”
- [x] ν”„λ΅μ νΈ λ””λ ‰ν† λ¦¬ κµ¬μ΅° μƒμ„±
- [x] requirements.txt μ‘μ„± (λ¨λ“  μμ΅΄μ„± ν¬ν•¨)
- [x] ν™κ²½ λ³€μ ν…ν”λ¦Ώ (.env.example)
- [x] .gitignore μ„¤μ •
- [x] Git μ €μ¥μ† μ΄κΈ°ν™”
- [x] README.md μ‘μ„±
- [x] QUICKSTART.md μ‘μ„±
- [x] setup.sh μλ™ν™” μ¤ν¬λ¦½νΈ

**ν•µμ‹¬ νμΌ:**
```
requirements.txt      # μ „μ²΄ ν¨ν‚¤μ§€ λ©λ΅
.env.example         # ν™κ²½ λ³€μ ν…ν”λ¦Ώ
setup.sh             # μλ™ μ„¤μΉ μ¤ν¬λ¦½νΈ
README.md            # ν”„λ΅μ νΈ κ°μ”
QUICKSTART.md        # λΉ λ¥Έ μ‹μ‘ κ°€μ΄λ“
```

### Phase 1: λ² μ΄μ¤ λ¨λΈ ν…μ¤νΈ
- [x] GPU ν™κ²½ ν™•μΈ μ¤ν¬λ¦½νΈ (`src/check_gpu.py`)
- [x] κΈ°λ³Έ LLM λ΅λ“ λ° μ¶”λ΅  ν…μ¤νΈ (`src/01_test_base_model.py`)
- [x] Gradio μΈν„°λ™ν‹°λΈ λ°λ¨ (`src/02_gradio_demo.py`)
- [x] μ„±λ¥ λ²¤μΉλ§ν¬ μ¤ν¬λ¦½νΈ (`src/03_benchmark.py`)

**μ£Όμ” κΈ°λ¥:**
- Full precision / 4-bit quantization μµμ…
- GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μΈ΅μ •
- Latency, Throughput λ²¤μΉλ§ν¬
- μ›Ή UI λ°λ¨

**ν…μ¤νΈ μ™„λ£:**
- macOS ν™κ²½μ—μ„ μ¤ν¬λ¦½νΈ μ‹¤ν–‰ κ²€μ¦
- Linux GPU μ„λ²„μ© μ½”λ“ μ¤€λΉ„ μ™„λ£

### Phase 2: Fine-tuning
- [x] λ°μ΄ν„°μ…‹ λ΅λ“ μ¤ν¬λ¦½νΈ (`src/data/01_load_dataset.py`)
  - κ³µκ° λ°μ΄ν„°μ…‹ λ‹¤μ΄λ΅λ“
  - λ°μ΄ν„° νƒμƒ‰ λ° λ¶„μ„
  - ν•™μµμ© ν¬λ§· λ³€ν™
- [x] ν•©μ„± λ°μ΄ν„° μƒμ„± (`src/data/02_generate_synthetic_data.py`)
  - MLOps/DevOps λ„λ©”μΈ νΉν™”
  - OpenAI API ν†µν•© (μ„ νƒ)
  - ν…ν”λ¦Ώ κΈ°λ° μƒμ„± (λ¬΄λ£)
- [x] LoRA fine-tuning (`src/train/01_lora_finetune.py`)
  - PEFT ν†µν•©
  - MLflow μ‹¤ν— μ¶”μ 
  - Instruction-following ν¬λ§·
- [x] QLoRA fine-tuning (`src/train/02_qlora_finetune.py`)
  - 4-bit μ–‘μν™”
  - λ©”λ¨λ¦¬ ν¨μ¨μ  ν•™μµ
  - MLflow ν†µν•©

**μ£Όμ” κΈ°λ¥:**
- μ—¬λ¬ λ°μ΄ν„°μ…‹ μ§€μ› (Alpaca, Dolly, etc.)
- μλ™ λ°μ΄ν„° μ „μ²λ¦¬
- ν•μ΄νΌνλΌλ―Έν„° μ»¤μ¤ν„°λ§μ΄μ§•
- MLflow μλ™ λ΅κΉ…
- ν•™μµ μ§„ν–‰ μƒν™© μ¶”μ 

---

## μ‘μ„±λ μ¤ν¬λ¦½νΈ λ©λ΅

### ν™κ²½ μ„¤μ •
```
setup.sh                          # μλ™ μ„¤μΉ
src/check_gpu.py                  # GPU ν™κ²½ ν™•μΈ
```

### Phase 1: λ² μ΄μ¤ λ¨λΈ
```
src/01_test_base_model.py         # λ¨λΈ λ΅λ“ λ° μ¶”λ΅ 
src/02_gradio_demo.py             # μ›Ή UI λ°λ¨
src/03_benchmark.py               # μ„±λ¥ λ²¤μΉλ§ν¬
```

### Phase 2: λ°μ΄ν„° λ° ν•™μµ
```
src/data/01_load_dataset.py       # λ°μ΄ν„°μ…‹ λ΅λ“
src/data/02_generate_synthetic_data.py  # ν•©μ„± λ°μ΄ν„° μƒμ„±
src/train/01_lora_finetune.py     # LoRA ν•™μµ
src/train/02_qlora_finetune.py    # QLoRA ν•™μµ
```

---

## μ§„ν–‰ μ¤‘/μμ • μ‘μ—… π§

### Phase 3: μµμ ν™” (μμ •)

**λ©ν‘:** κ³ μ„±λ¥ μ„λΉ™ λ° ν”„λ΅¬ν”„νΈ μµμ ν™”

**μ‘μ—… ν•­λ©:**
- [ ] vLLM μ„λ²„ κµ¬μ¶•
  - `src/serve/01_vllm_server.py`
  - κ³ μ† μ¶”λ΅  μ„λ²„
  - λ°°μΉ μ²λ¦¬ μµμ ν™”
- [ ] Prompt Engineering
  - `src/serve/02_prompt_templates.py`
  - ν…ν”λ¦Ώ μ‹μ¤ν…
  - Few-shot learning
- [ ] LangChain ν†µν•©
  - `src/serve/03_langchain_pipeline.py`
  - RAG (Retrieval-Augmented Generation)
  - Chain κµ¬μ„±
- [ ] μ„±λ¥ μµμ ν™”
  - μΊμ‹± μ „λµ
  - μ”μ²­ λ°°μΉ­
  - KV μΊμ‹ μµμ ν™”

**μμƒ μ†μ” μ‹κ°„:** 5-7μΌ

### Phase 4: ν”„λ΅λ•μ…ν™” (μμ •)

**λ©ν‘:** μ‹¤μ „ λ°°ν¬ κ°€λ¥ν• μ‹μ¤ν… κµ¬μ¶•

**μ‘μ—… ν•­λ©:**
- [ ] FastAPI λ°±μ—”λ“
  - `src/serve/04_fastapi_server.py`
  - RESTful API
  - μ¤νΈλ¦¬λ° μ‘λ‹µ
  - μΈμ¦/κ¶ν• κ΄€λ¦¬
- [ ] Docker μ»¨ν…μ΄λ„ν™”
  - `deployment/docker/Dockerfile.train`
  - `deployment/docker/Dockerfile.serve`
  - `deployment/docker/docker-compose.yml`
- [ ] λ¨λ‹ν„°λ§ μ‹μ¤ν…
  - Prometheus λ©”νΈλ¦­ μμ§‘
  - Grafana λ€μ‹λ³΄λ“
  - μ•λ¦Ό μ„¤μ •
- [ ] CI/CD νμ΄ν”„λΌμΈ
  - `.github/workflows/train.yml`
  - `.github/workflows/deploy.yml`
  - μλ™ ν…μ¤νΈ
  - μλ™ λ°°ν¬

**μμƒ μ†μ” μ‹κ°„:** 5-7μΌ

---

## ν”„λ΅μ νΈ κµ¬μ΅°

```
mlops-project/
β”β”€β”€ README.md                    # ν”„λ΅μ νΈ κ°μ”
β”β”€β”€ QUICKSTART.md                # λΉ λ¥Έ μ‹μ‘ κ°€μ΄λ“
β”β”€β”€ PROJECT_STATUS.md            # ν„μ¬ λ¬Έμ„
β”β”€β”€ requirements.txt             # ν¨ν‚¤μ§€ λ©λ΅
β”β”€β”€ setup.sh                     # μλ™ μ„¤μΉ
β”β”€β”€ .env.example                 # ν™κ²½ λ³€μ ν…ν”λ¦Ώ
β”β”€β”€ .gitignore                   # Git μ μ™Έ νμΌ
β”‚
β”β”€β”€ data/                        # λ°μ΄ν„°
β”‚   β”β”€β”€ raw/                     # μ›λ³Έ λ°μ΄ν„°
β”‚   β”β”€β”€ processed/               # μ „μ²λ¦¬ λ°μ΄ν„°
β”‚   β””β”€β”€ synthetic_train.json     # ν•©μ„± λ°μ΄ν„°
β”‚
β”β”€β”€ models/                      # λ¨λΈ μ €μ¥μ†
β”‚   β”β”€β”€ base/                    # μ›λ³Έ λ¨λΈ (μΊμ‹)
β”‚   β””β”€β”€ fine-tuned/              # Fine-tuned λ¨λΈ
β”‚       β”β”€β”€ lora-mistral-custom/
β”‚       β””β”€β”€ qlora-mistral-custom/
β”‚
β”β”€β”€ src/                         # μ†μ¤ μ½”λ“
β”‚   β”β”€β”€ check_gpu.py             # β… GPU ν™•μΈ
β”‚   β”β”€β”€ 01_test_base_model.py    # β… λ¨λΈ ν…μ¤νΈ
β”‚   β”β”€β”€ 02_gradio_demo.py        # β… Gradio λ°λ¨
β”‚   β”β”€β”€ 03_benchmark.py          # β… λ²¤μΉλ§ν¬
β”‚   β”‚
β”‚   β”β”€β”€ data/                    # λ°μ΄ν„° νμ΄ν”„λΌμΈ
β”‚   β”‚   β”β”€β”€ 01_load_dataset.py   # β… λ°μ΄ν„° λ΅λ“
β”‚   β”‚   β””β”€β”€ 02_generate_synthetic_data.py  # β… ν•©μ„± λ°μ΄ν„°
β”‚   β”‚
β”‚   β”β”€β”€ train/                   # ν•™μµ
β”‚   β”‚   β”β”€β”€ 01_lora_finetune.py  # β… LoRA
β”‚   β”‚   β””β”€β”€ 02_qlora_finetune.py # β… QLoRA
β”‚   β”‚
β”‚   β”β”€β”€ serve/                   # μ„λΉ™ (μμ •)
β”‚   β”‚   β”β”€β”€ 01_vllm_server.py    # π§ vLLM
β”‚   β”‚   β”β”€β”€ 02_prompt_templates.py  # π§ Prompts
β”‚   β”‚   β”β”€β”€ 03_langchain_pipeline.py  # π§ LangChain
β”‚   β”‚   β””β”€β”€ 04_fastapi_server.py    # π§ FastAPI
β”‚   β”‚
β”‚   β””β”€β”€ evaluate/                # ν‰κ°€ (μμ •)
β”‚       β””β”€β”€ 01_model_eval.py     # π§ ν‰κ°€ μ¤ν¬λ¦½νΈ
β”‚
β”β”€β”€ notebooks/                   # Jupyter λ…ΈνΈλ¶
β”‚   β””β”€β”€ experiments.ipynb        # π§ μ‹¤ν— λ…ΈνΈλ¶
β”‚
β”β”€β”€ configs/                     # μ„¤μ • νμΌ
β”‚   β””β”€β”€ train_config.yaml        # π§ ν•™μµ μ„¤μ •
β”‚
β”β”€β”€ tests/                       # ν…μ¤νΈ
β”‚   β””β”€β”€ test_*.py                # π§ λ‹¨μ„ ν…μ¤νΈ
β”‚
β””β”€β”€ deployment/                  # λ°°ν¬
    β”β”€β”€ docker/                  # Docker
    β”‚   β”β”€β”€ Dockerfile.train     # π§ ν•™μµμ©
    β”‚   β”β”€β”€ Dockerfile.serve     # π§ μ„λΉ™μ©
    β”‚   β””β”€β”€ docker-compose.yml   # π§ Compose
    β”‚
    β”β”€β”€ k8s/                     # Kubernetes (μ„ νƒ)
    β”‚   β””β”€β”€ *.yaml               # π§ Manifests
    β”‚
    β””β”€β”€ scripts/                 # λ°°ν¬ μ¤ν¬λ¦½νΈ
        β””β”€β”€ deploy.sh            # π§ λ°°ν¬ μλ™ν™”

β””β”€β”€ mlruns/                      # MLflow μ‹¤ν— λ΅κ·Έ
```

**λ²”λ΅€:**
- β… μ™„λ£
- π§ μμ •

---

## λ‹¤μ μ‹¤ν–‰ λ‹¨κ³„

### μ¦‰μ‹ μ‹¤ν–‰ κ°€λ¥ (GPU μ„λ²„μ—μ„)

1. **ν™κ²½ μ„¤μ •**
   ```bash
   ./setup.sh
   python src/check_gpu.py
   ```

2. **λ² μ΄μ¤ λ¨λΈ ν…μ¤νΈ**
   ```bash
   python src/01_test_base_model.py
   ```

3. **λ°μ΄ν„° μ¤€λΉ„**
   ```bash
   # μµμ… A: κ³µκ° λ°μ΄ν„°μ…‹
   python src/data/01_load_dataset.py

   # μµμ… B: ν•©μ„± λ°μ΄ν„°
   python src/data/02_generate_synthetic_data.py
   ```

4. **Fine-tuning**
   ```bash
   # λ©”λ¨λ¦¬κ°€ μ¶©λ¶„ν• κ²½μ° (14GB+ VRAM)
   python src/train/01_lora_finetune.py

   # λ©”λ¨λ¦¬κ°€ λ¶€μ΅±ν• κ²½μ° (4GB+ VRAM)
   python src/train/02_qlora_finetune.py
   ```

5. **μ‹¤ν— ν™•μΈ**
   ```bash
   mlflow ui
   # http://localhost:5000
   ```

---

## ν•™μµ ν¬μΈνΈ

μ΄ ν”„λ΅μ νΈλ¥Ό ν†µν•΄ μµλ“ν•  μ μλ” κΈ°μ :

### MLOps ν•µμ‹¬ μ—­λ‰
1. **λ¨λΈ κ°λ°**
   - LLM μ•„ν‚¤ν…μ² μ΄ν•΄
   - Fine-tuning κΈ°λ²• (LoRA, QLoRA)
   - ν•μ΄νΌνλΌλ―Έν„° νλ‹

2. **μ‹¤ν— κ΄€λ¦¬**
   - MLflow μ‹¤ν— μ¶”μ 
   - λ©”νΈλ¦­ λ΅κΉ…
   - λ¨λΈ λ²„μ €λ‹

3. **λ°μ΄ν„° νμ΄ν”„λΌμΈ**
   - λ°μ΄ν„°μ…‹ νλ μ΄μ…
   - μ „μ²λ¦¬ μλ™ν™”
   - ν•©μ„± λ°μ΄ν„° μƒμ„±

4. **μµμ ν™”**
   - μ–‘μν™” (4-bit, 8-bit)
   - λ©”λ¨λ¦¬ μµμ ν™”
   - μ¶”λ΅  μ†λ„ κ°μ„ 

5. **λ°°ν¬ (μμ •)**
   - μ„λΉ™ μΈν”„λΌ (vLLM, FastAPI)
   - μ»¨ν…μ΄λ„ν™” (Docker)
   - λ¨λ‹ν„°λ§ (Prometheus, Grafana)

---

## λ¦¬μ†μ¤ μ”κµ¬μ‚¬ν•­

### κ°λ° ν™κ²½
- **μµμ†:** CPU, 16GB RAM, 50GB λ””μ¤ν¬
- **κ¶μ¥:** GPU (4GB+ VRAM), 32GB RAM, 100GB λ””μ¤ν¬
- **μµμ :** GPU (16GB+ VRAM), 64GB RAM, 200GB λ””μ¤ν¬

### μ†ν”„νΈμ›¨μ–΄
- Python 3.10+
- CUDA 11.8+ (GPU μ‚¬μ© μ‹)
- Docker (λ°°ν¬ μ‹)
- Git

---

## μ°Έκ³  μλ£

### κ³µμ‹ λ¬Έμ„
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- [PEFT (LoRA/QLoRA)](https://huggingface.co/docs/peft)
- [MLflow](https://mlflow.org/docs/latest/)
- [vLLM](https://vllm.readthedocs.io/)

### λ…Όλ¬Έ
- LoRA: [arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)
- QLoRA: [arxiv.org/abs/2305.14314](https://arxiv.org/abs/2305.14314)

### μ»¤λ®¤λ‹ν‹°
- HuggingFace Discord
- MLOps Community Slack

---

## λ²„μ „ νμ¤ν† λ¦¬

### v0.1 (2025-12-11)
- β… Phase 0: ν”„λ΅μ νΈ μ΄κΈ°ν™”
- β… Phase 1: λ² μ΄μ¤ λ¨λΈ ν…μ¤νΈ μ¤ν¬λ¦½νΈ
- β… Phase 2: Fine-tuning μ¤ν¬λ¦½νΈ
- π“ λ¬Έμ„ μ‘μ„± (README, QUICKSTART, PROJECT_STATUS)

### v0.2 (μμ •)
- π§ Phase 3: μµμ ν™” (vLLM, LangChain)
- π§ ν‰κ°€ μ¤ν¬λ¦½νΈ
- π§ Jupyter λ…ΈνΈλ¶

### v1.0 (μμ •)
- π§ Phase 4: ν”„λ΅λ•μ…ν™”
- π§ Docker λ°°ν¬
- π§ λ¨λ‹ν„°λ§ μ‹μ¤ν…
- π§ CI/CD νμ΄ν”„λΌμΈ

---

**ν”„λ΅μ νΈ μ§„ν–‰λ¥ :** Phase 2 μ™„λ£ (50% μ™„μ„±)

**λ‹¤μ λ§μΌμ¤ν†¤:** Phase 3 μ‹μ‘ (vLLM μ„λΉ™)
