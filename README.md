# MLOps Chatbot Project

GPU ìì›ì„ í™œìš©í•œ ì»¤ìŠ¤í…€ ì±—ë´‡ êµ¬ì¶• í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. LLM Fine-tuningë¶€í„° í”„ë¡œë•ì…˜ ë°°í¬ê¹Œì§€ ì „ì²´ MLOps íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

## í”„ë¡œì íŠ¸ ëª©í‘œ

1. **LLM Fine-tuning ì‹¤ë¬´ ê²½í—˜**: LoRA/QLoRAë¥¼ í™œìš©í•œ íš¨ìœ¨ì  í•™ìŠµ
2. **ê³ ì„±ëŠ¥ ì„œë¹™**: vLLMì„ ì´ìš©í•œ ìµœì í™”ëœ ì¶”ë¡ 
3. **End-to-End MLOps**: ë°ì´í„° ìˆ˜ì§‘ â†’ í•™ìŠµ â†’ ë°°í¬ â†’ ëª¨ë‹ˆí„°ë§
4. **í”„ë¡œë•ì…˜ í™˜ê²½ êµ¬ì¶•**: FastAPI + Docker + CI/CD

## ê¸°ìˆ  ìŠ¤íƒ

### Core ML
- **Base Model**: **LLaMA-3-70B-Instruct** (í˜„ì¬) / Mistral-7B-Instruct / LLaMA-2-7B
- **Fine-tuning**: LoRA, QLoRA (PEFT)
- **Framework**: PyTorch, Transformers, Accelerate
- **Hardware**: RTX 5090 (31GB) + RTX 5060 Ti (15GB)

### MLOps Tools
- **Experiment Tracking**: MLflow
- **Data Versioning**: DVC
- **Serving**: vLLM, FastAPI
- **Orchestration**: LangChain

### DevOps
- **Containerization**: Docker
- **Monitoring**: Prometheus + Grafana
- **CI/CD**: GitHub Actions (ì˜ˆì •)

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
mlops-project/
â”œâ”€â”€ data/                    # ë°ì´í„°ì…‹
â”‚   â”œâ”€â”€ raw/                # ì›ë³¸ ë°ì´í„°
â”‚   â””â”€â”€ processed/          # ì „ì²˜ë¦¬ëœ ë°ì´í„°
â”œâ”€â”€ models/                  # ëª¨ë¸ ì €ì¥ì†Œ
â”‚   â”œâ”€â”€ base/               # ì‚¬ì „í•™ìŠµ ëª¨ë¸
â”‚   â””â”€â”€ fine-tuned/         # Fine-tuned ëª¨ë¸
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/               # ë°ì´í„° íŒŒì´í”„ë¼ì¸
â”‚   â”œâ”€â”€ train/              # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ serve/              # ì„œë¹™ ê´€ë ¨
â”‚   â””â”€â”€ evaluate/           # í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ notebooks/              # ì‹¤í—˜ìš© ë…¸íŠ¸ë¶
â”œâ”€â”€ configs/                # ì„¤ì • íŒŒì¼
â”œâ”€â”€ tests/                  # í…ŒìŠ¤íŠ¸ ì½”ë“œ
â”œâ”€â”€ deployment/             # ë°°í¬ ê´€ë ¨
â”‚   â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ k8s/
â”‚   â””â”€â”€ scripts/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ì‹œì‘í•˜ê¸°

### 1. í™˜ê²½ ì¤€ë¹„

```bash
# Python ê°€ìƒí™˜ê²½ ìƒì„± (Python 3.10+ í•„ìš”)
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install --upgrade pip
pip install -r requirements.txt
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
cp .env.example .env
# .env íŒŒì¼ì„ í¸ì§‘í•˜ì—¬ í•„ìš”í•œ í† í° ì…ë ¥
```

### 3. GPU í™˜ê²½ í™•ì¸

```bash
python src/check_gpu.py
```

## Phaseë³„ ê°€ì´ë“œ

### Phase 0: í™˜ê²½ ì¤€ë¹„ âœ… ì™„ë£Œ
- [x] í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±
- [x] ê°€ìƒí™˜ê²½ ì„¤ì •
- [x] requirements.txt ì‘ì„±
- [x] GPU í™˜ê²½ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
- [x] GPU í™˜ê²½ ê²€ì¦ (RTX 5090 + RTX 5060 Ti)
- [x] LLaMA-3-70B ëª¨ë¸ ì„¤ì •

### Phase 1: ë² ì´ìŠ¤ ì±—ë´‡ âœ… ì™„ë£Œ
- [x] LLaMA-3-8B ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
- [x] ê¸°ë³¸ LLM ë¡œë“œ ë° ì¶”ë¡  í…ŒìŠ¤íŠ¸ (Full precision)
- [x] Gradio UI ë°ëª¨ (http://localhost:7860)
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ì„ íƒì‚¬í•­)

### Phase 2: Fine-tuning ğŸ”„ ë‹¤ìŒ ë‹¨ê³„
- [ ] í•™ìŠµ ë°ì´í„° ì¤€ë¹„
- [ ] í•©ì„± ë°ì´í„° ìƒì„±
- [ ] LoRA fine-tuning
- [ ] QLoRA fine-tuning (4-bit)
- [ ] MLflow ì‹¤í—˜ ì¶”ì 

### Phase 3: ìµœì í™” (5-7ì¼)
- [ ] vLLM ì„œë¹™ êµ¬ì¶•
- [ ] Prompt Engineering
- [ ] LangChain íŒŒì´í”„ë¼ì¸
- [ ] ì„±ëŠ¥ ìµœì í™”

### Phase 4: í”„ë¡œë•ì…˜í™” (5-7ì¼)
- [ ] FastAPI ë°±ì—”ë“œ
- [ ] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
- [ ] Docker ì»¨í…Œì´ë„ˆí™”
- [ ] ëª¨ë‹ˆí„°ë§ (Prometheus + Grafana)
- [ ] CI/CD íŒŒì´í”„ë¼ì¸

## í•„ìˆ˜ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´
- **GPU**: NVIDIA GPU 16GB+ VRAM (ê¶Œì¥: A100, A10, RTX 3090/4090)
- **RAM**: 32GB+ ê¶Œì¥
- **Storage**: 50GB+ ì—¬ìœ  ê³µê°„ (ëª¨ë¸ ì €ì¥ìš©)

### ì†Œí”„íŠ¸ì›¨ì–´
- Python 3.10+
- CUDA 11.8+ (GPU ì‚¬ìš© ì‹œ)
- Docker (ë°°í¬ ì‹œ)
- Git

### API í‚¤ (ì„ íƒ)
- HuggingFace Token (Gated ëª¨ë¸ ì‚¬ìš© ì‹œ)
- OpenAI API Key (í•©ì„± ë°ì´í„° ìƒì„± ì‹œ)

## ì£¼ìš” ëª…ë ¹ì–´

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# ë² ì´ìŠ¤ ëª¨ë¸ í…ŒìŠ¤íŠ¸
python src/01_test_base_model.py

# Gradio ë°ëª¨ ì‹¤í–‰
python src/02_gradio_demo.py

# Fine-tuning
python src/train/01_lora_finetune.py

# MLflow UI
mlflow ui

# vLLM ì„œë²„ ì‹¤í–‰
python -m vllm.entrypoints.api_server \
  --model ./models/fine-tuned/lora-mistral-custom \
  --port 8000
```

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Out of Memory (OOM)
- Batch size ì¤„ì´ê¸°
- QLoRA ì‚¬ìš© (4-bit ì–‘ìí™”)
- Gradient checkpointing í™œì„±í™”

### ëŠë¦° í•™ìŠµ ì†ë„
- Mixed precision (fp16/bf16) ì‚¬ìš©
- Gradient accumulation í™œìš©
- Flash Attention ì ìš©

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨
- HuggingFace í† í° í™•ì¸
- ì¸í„°ë„· ì—°ê²° í™•ì¸
- ìºì‹œ ë””ë ‰í† ë¦¬ ê¶Œí•œ í™•ì¸

## ì°¸ê³  ìë£Œ

- [Transformers Documentation](https://huggingface.co/docs/transformers)
- [PEFT Documentation](https://huggingface.co/docs/peft)
- [vLLM Documentation](https://vllm.readthedocs.io/)
- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [LangChain Documentation](https://python.langchain.com/)
