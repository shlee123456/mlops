#!/usr/bin/env python3
"""
Phase 1-2: Gradio ì¸í„°ë™í‹°ë¸Œ ë°ëª¨

ì›¹ UIë¥¼ í†µí•´ ëª¨ë¸ê³¼ ëŒ€í™”í•  ìˆ˜ ìˆëŠ” ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import torch
import gradio as gr
from dotenv import load_dotenv
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    pipeline
)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ì „ì—­ ë³€ìˆ˜
model = None
tokenizer = None
device = None


def initialize_model(model_name, use_quantization=True):
    """ëª¨ë¸ ì´ˆê¸°í™”"""
    global model, tokenizer, device

    # ë””ë°”ì´ìŠ¤ í™•ì¸
    if torch.cuda.is_available():
        device = "cuda"
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = "mps"
    else:
        device = "cpu"

    print(f"Using device: {device}")

    # Tokenizer ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        token=os.getenv("HUGGINGFACE_TOKEN")
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # ëª¨ë¸ ë¡œë“œ
    if use_quantization and device == "cuda":
        print("Loading model with 4-bit quantization...")
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16
        )

        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            device_map="auto",
            token=os.getenv("HUGGINGFACE_TOKEN")
        )
    else:
        print("Loading model in full precision...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            token=os.getenv("HUGGINGFACE_TOKEN")
        )

        if device != "cuda":
            model = model.to(device)

    print("âœ“ Model loaded successfully!")


def generate_response(
    message,
    history,
    max_new_tokens=256,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.1
):
    """
    ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•œ ì‘ë‹µ ìƒì„±

    Args:
        message: ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€
        history: ëŒ€í™” ê¸°ë¡ [[user, bot], [user, bot], ...]
        max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
        temperature: ìƒ˜í”Œë§ ì˜¨ë„
        top_p: Top-p ìƒ˜í”Œë§
        repetition_penalty: ë°˜ë³µ íŒ¨ë„í‹°
    """
    if model is None:
        return "âš  Model not loaded. Please check initialization."

    # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    conversation = ""
    for user_msg, bot_msg in history:
        conversation += f"User: {user_msg}\nAssistant: {bot_msg}\n"
    conversation += f"User: {message}\nAssistant:"

    # ì…ë ¥ í† í°í™”
    inputs = tokenizer(conversation, return_tensors="pt")

    if device == "cuda":
        inputs = inputs.to("cuda")
    elif device == "mps":
        inputs = inputs.to("mps")

    # ì‘ë‹µ ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )

    # ì‘ë‹µ ë””ì½”ë”©
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # "Assistant:" ì´í›„ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    response = generated_text.split("Assistant:")[-1].strip()

    # ë‹¤ìŒ "User:" ì „ê¹Œì§€ë§Œ ì¶”ì¶œ
    if "User:" in response:
        response = response.split("User:")[0].strip()

    return response


def create_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""

    # ChatInterface ìƒì„± (Gradio ìµœì‹  ë²„ì „ í˜¸í™˜)
    demo = gr.ChatInterface(
        fn=generate_response,
        title="ğŸ¤– MLOps Chatbot Demo",
        description="""
        ì‚¬ì „í•™ìŠµëœ LLMê³¼ ëŒ€í™”í•  ìˆ˜ ìˆëŠ” ë°ëª¨ì…ë‹ˆë‹¤.
        ì•„ë˜ ì„¤ì •ì„ ì¡°ì •í•˜ì—¬ ì‘ë‹µ ìŠ¤íƒ€ì¼ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """,
        examples=[
            ["What is MLOps?", 256, 0.7, 0.9, 1.1],
            ["Explain machine learning in simple terms.", 256, 0.7, 0.9, 1.1],
            ["Write a Python function to sort a list.", 256, 0.7, 0.9, 1.1],
            ["What are the benefits of CI/CD?", 256, 0.7, 0.9, 1.1],
        ],
        additional_inputs=[
            gr.Slider(
                minimum=50,
                maximum=512,
                value=256,
                step=1,
                label="Max New Tokens",
                info="ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜"
            ),
            gr.Slider(
                minimum=0.1,
                maximum=2.0,
                value=0.7,
                step=0.1,
                label="Temperature",
                info="ë†’ì„ìˆ˜ë¡ ì°½ì˜ì , ë‚®ì„ìˆ˜ë¡ ê²°ì •ì "
            ),
            gr.Slider(
                minimum=0.1,
                maximum=1.0,
                value=0.9,
                step=0.05,
                label="Top-p",
                info="ëˆ„ì  í™•ë¥  ì„ê³„ê°’"
            ),
            gr.Slider(
                minimum=1.0,
                maximum=2.0,
                value=1.1,
                step=0.1,
                label="Repetition Penalty",
                info="ë°˜ë³µ ë°©ì§€ ì •ë„"
            ),
        ],
    )

    return demo


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("  Phase 1-2: Gradio Interactive Demo")
    print("="*60 + "\n")

    # ëª¨ë¸ ì´ë¦„
    model_name = os.getenv("BASE_MODEL_NAME", "mistralai/Mistral-7B-Instruct-v0.2")

    # ì‚¬ìš©ì ì„ íƒ
    print(f"Model: {model_name}")
    print("\nQuantization options:")
    print("  1) Use 4-bit quantization (recommended, ~4GB VRAM)")
    print("  2) Full precision (~14GB VRAM)")

    if not torch.cuda.is_available():
        print("\nâš  No CUDA GPU detected. Using full precision.")
        use_quantization = False
    else:
        choice = input("\nEnter choice (1-2): ").strip()
        use_quantization = (choice == "1")

    # ëª¨ë¸ ì´ˆê¸°í™”
    try:
        print("\nInitializing model...")
        initialize_model(model_name, use_quantization)
    except Exception as e:
        print(f"\nâœ— Error loading model: {e}")
        print("\nTroubleshooting:")
        print("  1. Check HuggingFace token in .env")
        print("  2. Verify model name")
        print("  3. Check GPU memory availability")
        return

    # Gradio ì¸í„°í˜ì´ìŠ¤ ì‹œì‘
    print("\n" + "="*60)
    print("Starting Gradio interface...")
    print("="*60 + "\n")

    demo = create_interface()

    # ì„œë²„ ì‹œì‘
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )


if __name__ == "__main__":
    main()
